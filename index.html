<!DOCTYPE html>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0"/>
  <link rel="icon" type="image/x-icon" href="images/YH.png" />
  <title>Project</title>
  <!--Import Google Icon Font-->

  <!--<link href="http://fonts.useso.com/icon?family=Material+Icons" rel="stylesheet">-->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <!--Import materialize.css-->
  <link type="text/css" rel="stylesheet" href="css/materialize.min.css"  media="screen,projection"/>
  <link type="text/css" rel="stylesheet" href="css/main.css">

  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <!--Import jQuery before materialize.js-->

  <script type="text/javascript" src="js/jquery-3.0.0.min.js"></script>
  <script type="text/javascript" src="js/materialize.min.js"></script>
  <script type="text/javascript" src="js/main.js"></script>
  <style>
    body
    {
      font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
    }
    .vertical-nav
    {
      margin: 0;
      position: fixed;
      width: 300px;
      /*background-color: #343131;*/
      min-height: 100%;
      background-image: url("images/vertical_oct24.jpg");
      background-repeat: round;
    }

    .profile-block
    {
      position: relative;
      height: 300px;
      /*background-color: #324D5C;*/
    }
    .profile-block-sm
    {
      position: relative;
      height: 200px;
      /*background-color: #324D5C;*/

    }
    .profile-block-tiny
    {
      position: relative;
      height: 50px;
      /*background-color: #324D5C;*/
    }

    .profile
    {
      position: absolute;
      top: 50%;
      transform: translateY(-50%);
      border-radius: 50%;
      margin-left: 30px;

      border: solid 5px #F4D03F;
    }

    .profile-sm
    {
      /*position: absolute;*/
      /*top: 50%;*/
      /*left: 50%;*/
      /*transform: translateX(50%);*/
      border-radius: 50%;
      margin: auto;

      border: solid 5px #F4D03F;
    }

    .profile-tiny
    {
      position: relative;
      /* top: 50%; */
      transform: translateY(-50%);
      border-radius: 50%;
      margin-left: 20px;

      border: solid 5px #F4D03F;
    }

    .logo-link
    {
      padding-left: 10px;
      padding-right: 10px;
    }
    .card-content
    {
      padding: 0 !important;
      font-size: large
    }

    .card-title
    {
      background-color: #FBD8B0;
      padding: 12px;
    }

    p
    {
      line-height: 150%;
      text-align: justify;
    }
    li
    {
      padding-bottom: 10px;
    }
    strong
    {
      font-weight: bolder;
    }
    .project-item
    {
    }
    .project-logo
    {
    }
  </style>
</head>
<body>
<div class="vertical-nav hide-on-med-and-down">
  <div class="profile-block">
    <img src="images/logo3.png" width="240px" class="profile">
  </div>
  <div style="color: #F8F8f8; font-size: large; margin-top: -30px" class="center-align">
    <p style="" class="center">Vencoders</p>
    <p style="font-size: medium;" class="center">Nanjing University of Information Science and Technology</p>
    <!-- <p style="padding-bottom: 30px; font-size: medium;" class="center"><i class="fa fa-envelope"></i> yyhu AT nyu&middot;edu</p> -->
<!--    <span class="logo-link">
            <a href="https://scholar.google.com/citations?user=MkWer14AAAAJ">
                <img src="images/google-scholar-logo.png" style="width: 30px">
            </a>
        </span>
    <span class="logo-link">
            <a href="https://instagram.com/minoshirod/">
                <img src="images/Instagram.png" style="width: 30px">
            </a>
        </span> -->
    <span class="logo-link">
            <a href="https://github.com/Vencoders">
                <img src="images/GitHub-Mark-Light-120px-plus.png" style="width: 30px">
            </a>
        </span>

<!--    <span class="logo-link">
            <a href="https://www.facebook.com/yueyuhu">
                <img src="images/FB-f-Logo__white_100.png" style="width: 30px">
            </a>
        </span> -->
  </div>

</div>


<div class="hide-on-med-and-down">
  <div style="padding-left: 300px">
    <div class="row" style="">
      <div class="container" style="width: 80%;">
        <div class="row">
          <div class="col s12 m12 l12">
            <div class="card">
              <div class="card-content" style="color: #30505D;">
                <span class="card-title" style="">About Ours</span>
                <p style="padding: 24px; font-size: large;">1.School of Computer Science, Nanjing University of Information Science and Technology, Nanjing, China, <br>
									2.Engineering Research Center of Digital Forensics, Ministry of Education, Nanjing University of Information Science and Technology, Nanjing, China.
                  
              </div>
            </div>
          </div>

         
          <div class="col s12 m12 l12">
            <div class="card">
              <div class="card-content" style="color: #30505D;">
                <span class="card-title" style="">Projects</span>
                <div class="project-item">
                  <table>

                    <tr>
                      <td style="width: 30%">
                        <div class="project-logo">
                          <a href="https://vencoders.github.io/natural-transfer.html"><img src="images/frame3.png" style="width: 100%"></a>
                        </div>
                      </td>
                      <td style="width: 70%;">
                        <div class="" >
                          <p style="font-size: 18pt; text-align: left;">
                            Neural texture transfer assisted video coding with adaptive up-sampling
                          </p>
                          <p>
                            A neural texture transfer-assisted video coding with an adaptive up-sampling scheme is proposed in this paper. This scheme adaptively decides whether a frame should be down-sampled or not. In  the decoder, the down-sampled frames are restored by exploring their correlations with the frames that are not down-sampled using neural texture transfer in a multi-scale manner.    
                          </p>
                          <p>
                            <a href="https://vencoders.github.io/natural-transfer">[Project]</a>
							<a href="https://weizequan.github.io/SPIC2022/paper.pdf">[PDF]</a>
							<a href="https://github.com/Vencoders/Ref-frame-encode">[Code]</a>
                          </p>
                        </div>
                      </td>
                    </tr>
					
					
					<tr>
					  <td style="width: 30%">
					    <div class="project-logo">
					      <a href="https://vencoders.github.io/hfcgcnn.html"><img src="images/frame4.png" style="width: 100%"></a>
					    </div>
					  </td>
					  <td style="width: 70%;">
					    <div class="" >
					      <p style="font-size: 18pt; text-align: left;">
					        High-frequency guided CNN for video compression artifacts reduction
					      </p>
					      <p>
					        This paper proposes the HFCG-CNN for video compression artifacts reduction, consisting of high-frequency guidance module and quality enhancement module. The high-frequency guidance module explicitly extracts the high-frequency information in the Y component. Then, the high-frequency information is used in the quality enhancement module to guide the recovery of all Y, U and V components. The experiment results demonstrate the effectiveness of the proposed HFCGCNN method.  
					      </p>
					      <p>
					        <a href="https://vencoders.github.io/hfcgcnn">[Project]</a>
							<a href="https://ieeexplore.ieee.org/abstract/document/10008814/">[PDF]</a>
							<a href="https://github.com/Vencoders/HFCG-CNN">[Code]</a>
					      </p>
					    </div>
					  </td>
					</tr>

          
          <tr>
					  <td style="width: 30%">
					    <div class="project-logo">
					      <a href="https://vencoders.github.io/mamiqa.html"><img src="images/mamiqa1.png" style="width: 100%"></a>
					    </div>
					  </td>
					  <td style="width: 70%;">
					    <div class="" >
					      <p style="font-size: 18pt; text-align: left;">
					        MAMIQA: No-Reference Image Quality Assessment based on Multiscale Attention Mechanism with Natural Scene Statistics
					      </p>
					      <p>
                  No-Reference Image Quality Assessment aims to evaluate the perceptual quality of an image, according to human perception. This paper propose a lightweight attention mechanism using decomposed large-kernel convolutions to extract multiscale features, and a novel feature enhancement module to simulate HVS. We also propose to compensate the information loss caused by image resizing, with supplementary features from natural scene statistics. Experimental results on five standard datasets show that the proposed method surpasses the SOTA, while significantly reducing the computational costs.
					      </p>
					      <p>
					        <a href="https://vencoders.github.io/mamiqa">[Project]</a>
							<a href="https://ieeexplore.ieee.org/document/10124974">[PDF]</a>
							<a href="https://github.com/Vencoders/MAMIQA">[Code]</a>
					      </p>
					    </div>
					  </td>
					</tr>

          <tr>
					  <td style="width: 30%">
					    <div class="project-logo">
					      <a href="https://vencoders.github.io/tvqe.html"><img src="images/tvqe.jpg" style="width: 100%"></a>
					    </div>
					  </td>
					  <td style="width: 70%;">
					    <div class="" >
					      <p style="font-size: 18pt; text-align: left;">
					        End-to-end Transformer for Compressed Video Quality Enhancement
					      </p>
					      <p>
					        we propose a Transformer-based compressed video quality enhancement(TVQE) method, consisting of Transformer based Spatio-Temporal feature Fusion (TSTF) module and Multi-scale Channel-wise Attention based Quality Enhancement (MCQE) module.
					      </p>
					      <p>
					        <a href="https://vencoders.github.io/tvqe.html">[Project]</a>
							<a href="https://arxiv.org/abs/2210.13827">[PDF]</a>
							<a href="https://github.com/Vencoders/TVQE">[Code]</a>
					      </p>
					    </div>
					  </td>
					</tr>

                      <tr>
                          <td style="width: 30%">
                              <div class="project-logo">
                                  <a href="https://vencoders.github.io/PII.html"><img src="images/f1.png" style="width: 100%"></a>
                              </div>
                          </td>
                          <td style="width: 70%;">
                              <div class="" >
                                  <p style="font-size: 18pt; text-align: left;">
                                      PANORAMIC IMAGE INPAINTING WITH GATED CONVOLUTION AND CONTEXTUAL RECONSTRUCTION LOSS
                                  </p>
                                  <p>
                                      we propose a panoramic image inpainting framework that consists of a Face Generator, a Cube Generator, a side branch, and two discriminators. We use the Cubemap Projection (CMP) format as network input.
                                      The generator employs gated convolutions to distinguish valid pixels from invalid ones, while a side branch is designed utilizing contextual reconstruction (CR) loss to guide the generators to find the most suitable reference patch for inpainting the missing region.

                                  </p>
                                  <p>
                                      <a href="https://vencoders.github.io/PII.html">[Project]</a>
                                      <a href="https://ieeexplore.ieee.org/document/10446469">[PDF]</a>
                                      <a href="https://github.com/Vencoders/Panoramic-image-inpainting">[Code]</a>
                                  </p>
                              </div>
                          </td>
                      </tr>
		      <tr>
                          <td style="width: 30%">
                              <div class="project-logo">
                                  <a href="https://vencoders.github.io/PU-AT.html"><img src="images/PU-AT1.png" style="width: 100%"></a>
                              </div>
                          </td>
                          <td style="width: 70%;">
                              <div class="" >
                                  <p style="font-size: 18pt; text-align: left;">
                                      Point cloud sampling method based on Transformer
                                  </p>
                                  <p>
                                      we propose a novel 3D point cloud up-sampling method based on Transformers aiming to explore the po-tential of Transformers in this domain. Initially, a two-step network is employed, transitioning from rough dense point cloud generation to point cloud refinement, with each sub-network focusing on specific objec-tives. The coarse generation network aims to increase point cloud resolution, utilizing multiple densely connected convolutional blocks to extract deep high-dimensional features. To efficiently explore the up-sampling space of point clouds, an innovative integration of upsampling module with Transformer is in-troduced. While the rough generation of point clouds increases the number of points, correction of noise points is essential. In the refinement network, an adaptive refinement module is introduced, capable of au-tonomously aggregating structural information based on local point cloud characteristics.

                                  </p>
                                  <p>
                                      <a href="https://vencoders.github.io/PU-AT.html">[Project]</a>
                                      <a href="">[PDF]</a>
                                      <a href="https://github.com/Vencoders/PU-AT">[Code]</a>
                                  </p>
                              </div>
                          </td>
                      </tr>
			  <tr>
                          <td style="width: 30%">
                              <div class="project-logo">
                                  <a href="https://vencoders.github.io/JICD.html"><img src="images/overviewjicd.png" style="width: 100%"></a>
                              </div>
                          </td>
                          <td style="width: 70%;">
                              <div class="" >
                                  <p style="font-size: 18pt; text-align: left;">
                                      JOINT END-TO-END IMAGE COMPRESSION AND DENOISING: LEVERAGING CONTRASTIVE LEARNING AND MULTI-SCALE SELF-ONNS
                                  </p>
                                  <p>
                                      Noisy images are a challenge to image compression algorithms due to the inherent difficulty of compressing noise. As noise cannot easily be discerned from image details, such as high-frequency signals, its presence leads to extra bits needed for compression. Since the emerging learned image compression paradigm enables end-to-end optimization of codecs, recent efforts were made to integrate denoising into the compression model, relying on clean image features to guide denoising. However, these methods exhibit suboptimal performance under high noise levels, lacking the capability to generalize across diverse noise types. In this paper, we propose a novel method integrating a multi-scale denoiser comprising of Self Organizing Operational Neural Networks, for joint image compression and denoising. We employ contrastive learning to boost the network ability to differentiate noise from high frequency signal components, by emphasizing the correlation between noisy and clean counterparts. Experimental results demonstrate the effectiveness of the proposed method both in ratedistortion performance, and codec speed, outperforming the current state-of-the-art.

                                  </p>
                                  <p>
                                      <a href="https://vencoders.github.io/JICD.html">[Project]</a>
                                      <a href="">[PDF]</a>
                                      <a href="https://github.com/Vencoders/JICD">[Code]</a>
                                  </p>
                              </div>
                          </td>
                      </tr>	  
			<tr>
                          <td style="width: 30%">
                              <div class="project-logo">
                                  <a href="https://vencoders.github.io/PU-DT.html"><img src="images/PU-DTframework.png" style="width: 100%"></a>
                              </div>
                          </td>
                          <td style="width: 70%;">
                              <div class="" >
                                  <p style="font-size: 18pt; text-align: left;">
                                      Point Cloud Upsampling via Implicit Shape Priors Discovery and Refinement
                                  </p>
                                  <p>
                                      The point clouds obtained by scanning sensors are often sparse and non-uniform, therefore, point cloud upsampling is of vital importance. This paper considers geometric priors as a rich source to guide point cloud generation for the better qualities. However, it is less flexible to explicitly exploit geometric priors of object surface, such as local geometric smoothness and fairness. In light of this, this paper proposes a novel two-stage method via discovering and exploiting implicit shape priors, which can consist of coarse point cloud upsampling and fine details refining. Specifically, at the first stage, we explore to discover geometric priors in an implicit manner via Dual Transformer, which simultaneously addressing local and global information during feature encoding, while a Neighborhood Refinement module is proposed to handle with geometric irregularities and noises via exploiting feature similarity of neighboring points. Extensive experiments on synthetic and real datasets can verify our motivation, as our method can gain superior performance to existing upsampling methods, especially with noisy point clouds. 

                                  </p>
                                  <p>
                                      <a href="https://vencoders.github.io/PU-DT.html">[Project]</a>
                                      <a href="">[PDF]</a>
                                      <a href="https://github.com/Vencoders/PU-DT">[Code]</a>
                                  </p>
                              </div>
                          </td>
                      </tr>	  
                  </table>
                </div>

              </div>
            </div>
          </div>


        </div>
      </div>
    </div>
  </div>
</div>


</div>
</body>
</html>
