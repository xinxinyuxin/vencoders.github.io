<!DOCTYPE html>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0"/>
  <link rel="icon" type="image/x-icon" href="images/YH.png" />
  <title>Project</title>
  <!--Import Google Icon Font-->

  <!--<link href="http://fonts.useso.com/icon?family=Material+Icons" rel="stylesheet">-->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <!--Import materialize.css-->
  <link type="text/css" rel="stylesheet" href="css/materialize.min.css"  media="screen,projection"/>
  <link type="text/css" rel="stylesheet" href="css/main.css">

  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <!--Import jQuery before materialize.js-->

  <script type="text/javascript" src="js/jquery-3.0.0.min.js"></script>
  <script type="text/javascript" src="js/materialize.min.js"></script>
  <script type="text/javascript" src="js/main.js"></script>
  <style>
    body
    {
      font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
    }
    .vertical-nav
    {
      margin: 0;
      position: fixed;
      width: 300px;
      /*background-color: #343131;*/
      min-height: 100%;
      background-image: url("images/vertical_oct24.jpg");
      background-repeat: round;
    }

    .profile-block
    {
      position: relative;
      height: 300px;
      /*background-color: #324D5C;*/
    }
    .profile-block-sm
    {
      position: relative;
      height: 200px;
      /*background-color: #324D5C;*/

    }
    .profile-block-tiny
    {
      position: relative;
      height: 50px;
      /*background-color: #324D5C;*/
    }

    .profile
    {
      position: absolute;
      top: 50%;
      transform: translateY(-50%);
      border-radius: 50%;
      margin-left: 30px;

      border: solid 5px #F4D03F;
    }

    .profile-sm
    {
      /*position: absolute;*/
      /*top: 50%;*/
      /*left: 50%;*/
      /*transform: translateX(50%);*/
      border-radius: 50%;
      margin: auto;

      border: solid 5px #F4D03F;
    }

    .profile-tiny
    {
      position: relative;
      /* top: 50%; */
      transform: translateY(-50%);
      border-radius: 50%;
      margin-left: 20px;

      border: solid 5px #F4D03F;
    }

    .logo-link
    {
      padding-left: 10px;
      padding-right: 10px;
    }
    .card-content
    {
      padding: 0 !important;
      font-size: large
    }

    .card-title
    {
      background-color: #FBD8B0;
      padding: 12px;
    }

    p
    {
      line-height: 150%;
      text-align: justify;
    }
    li
    {
      padding-bottom: 10px;
    }
    strong
    {
      font-weight: bolder;
    }
    .project-item
    {
    }
    .project-logo
    {
    }
  </style>
</head>
<body>
<div class="vertical-nav hide-on-med-and-down">
  <div class="profile-block">
    <img src="images/logo3.png" width="240px" class="profile">
  </div>
  <div style="color: #F8F8f8; font-size: large; margin-top: -30px" class="center-align">
    <p style="" class="center">Vencoders</p>
    <p style="font-size: medium;" class="center">Nanjing University of Information Science and Technology</p>
    <!-- <p style="padding-bottom: 30px; font-size: medium;" class="center"><i class="fa fa-envelope"></i> yyhu AT nyu&middot;edu</p> -->
<!--    <span class="logo-link">
            <a href="https://scholar.google.com/citations?user=MkWer14AAAAJ">
                <img src="images/google-scholar-logo.png" style="width: 30px">
            </a>
        </span>
    <span class="logo-link">
            <a href="https://instagram.com/minoshirod/">
                <img src="images/Instagram.png" style="width: 30px">
            </a>
        </span> -->
    <span class="logo-link">
            <a href="https://github.com/Vencoders">
                <img src="images/GitHub-Mark-Light-120px-plus.png" style="width: 30px">
            </a>
        </span>

<!--    <span class="logo-link">
            <a href="https://www.facebook.com/yueyuhu">
                <img src="images/FB-f-Logo__white_100.png" style="width: 30px">
            </a>
        </span> -->
  </div>

</div>


<div class="hide-on-med-and-down">
  <div style="padding-left: 300px">
    <div class="row" style="">
      <div class="container" style="width: 80%;">
        <div class="row">
          <div class="col s12 m12 l12">
            <div class="card">
              <div class="card-content" style="color: #30505D;">
                <span class="card-title" style="">About Ours</span>
                <p style="padding: 24px; font-size: large;">1.School of Computer Science, Nanjing University of Information Science and Technology, Nanjing, China, <br>
									2.Engineering Research Center of Digital Forensics, Ministry of Education, Nanjing University of Information Science and Technology, Nanjing, China.
                  
              </div>
            </div>
          </div>

         
          <div class="col s12 m12 l12">
            <div class="card">
              <div class="card-content" style="color: #30505D;">
                <span class="card-title" style="">Projects</span>
                <div class="project-item">
                  <table>

                    <tr>
                      <td style="width: 30%">
                        <div class="project-logo">
                          <a href="https://vencoders.github.io/natural-transfer.html"><img src="images/frame3.png" style="width: 100%"></a>
                        </div>
                      </td>
                      <td style="width: 70%;">
                        <div class="" >
                          <p style="font-size: 18pt; text-align: left;">
                            Neural texture transfer assisted video coding with adaptive up-sampling
                          </p>
                          <p>
                            A neural texture transfer-assisted video coding with an adaptive up-sampling scheme is proposed in this paper. This scheme adaptively decides whether a frame should be down-sampled or not. In  the decoder, the down-sampled frames are restored by exploring their correlations with the frames that are not down-sampled using neural texture transfer in a multi-scale manner.    
                          </p>
                          <p>
                            <a href="https://vencoders.github.io/natural-transfer">[Project]</a>
							<a href="https://weizequan.github.io/SPIC2022/paper.pdf">[PDF]</a>
							<a href="https://github.com/Vencoders/Ref-frame-encode">[Code]</a>
                          </p>
                        </div>
                      </td>
                    </tr>
					
					
					<tr>
					  <td style="width: 30%">
					    <div class="project-logo">
					      <a href="https://vencoders.github.io/hfcgcnn.html"><img src="images/frame4.png" style="width: 100%"></a>
					    </div>
					  </td>
					  <td style="width: 70%;">
					    <div class="" >
					      <p style="font-size: 18pt; text-align: left;">
					        High-frequency guided CNN for video compression artifacts reduction
					      </p>
					      <p>
					        This paper proposes the HFCG-CNN for video compression artifacts reduction, consisting of high-frequency guidance module and quality enhancement module. The high-frequency guidance module explicitly extracts the high-frequency information in the Y component. Then, the high-frequency information is used in the quality enhancement module to guide the recovery of all Y, U and V components. The experiment results demonstrate the effectiveness of the proposed HFCGCNN method.  
					      </p>
					      <p>
					        <a href="https://vencoders.github.io/hfcgcnn">[Project]</a>
							<a href="https://ieeexplore.ieee.org/abstract/document/10008814/">[PDF]</a>
							<a href="https://github.com/Vencoders/HFCG-CNN">[Code]</a>
					      </p>
					    </div>
					  </td>
					</tr>

          
          <tr>
					  <td style="width: 30%">
					    <div class="project-logo">
					      <a href="https://vencoders.github.io/mamiqa.html"><img src="images/mamiqa1.png" style="width: 100%"></a>
					    </div>
					  </td>
					  <td style="width: 70%;">
					    <div class="" >
					      <p style="font-size: 18pt; text-align: left;">
					        MAMIQA: No-Reference Image Quality Assessment based on Multiscale Attention Mechanism with Natural Scene Statistics
					      </p>
					      <p>
                  No-Reference Image Quality Assessment aims to evaluate the perceptual quality of an image, according to human perception. This paper propose a lightweight attention mechanism using decomposed large-kernel convolutions to extract multiscale features, and a novel feature enhancement module to simulate HVS. We also propose to compensate the information loss caused by image resizing, with supplementary features from natural scene statistics. Experimental results on five standard datasets show that the proposed method surpasses the SOTA, while significantly reducing the computational costs.
					      </p>
					      <p>
					        <a href="https://vencoders.github.io/mamiqa">[Project]</a>
							<a href="https://ieeexplore.ieee.org/document/10124974">[PDF]</a>
							<a href="https://github.com/Vencoders/MAMIQA">[Code]</a>
					      </p>
					    </div>
					  </td>
					</tr>

          <tr>
					  <td style="width: 30%">
					    <div class="project-logo">
					      <a href="https://vencoders.github.io/tvqe.html"><img src="images/tvqe.jpg" style="width: 100%"></a>
					    </div>
					  </td>
					  <td style="width: 70%;">
					    <div class="" >
					      <p style="font-size: 18pt; text-align: left;">
					        End-to-end Transformer for Compressed Video Quality Enhancement
					      </p>
					      <p>
					        we propose a Transformer-based compressed video quality enhancement(TVQE) method, consisting of Transformer based Spatio-Temporal feature Fusion (TSTF) module and Multi-scale Channel-wise Attention based Quality Enhancement (MCQE) module.
					      </p>
					      <p>
					        <a href="https://vencoders.github.io/tvqe.html">[Project]</a>
							<a href="https://arxiv.org/abs/2210.13827">[PDF]</a>
							<a href="https://github.com/Vencoders/TVQE">[Code]</a>
					      </p>
					    </div>
					  </td>
					</tr>

                      <tr>
                          <td style="width: 30%">
                              <div class="project-logo">
                                  <a href="https://vencoders.github.io/PII.html"><img src="images/f1.png" style="width: 100%"></a>
                              </div>
                          </td>
                          <td style="width: 70%;">
                              <div class="" >
                                  <p style="font-size: 18pt; text-align: left;">
                                      PANORAMIC IMAGE INPAINTING WITH GATED CONVOLUTION AND CONTEXTUAL RECONSTRUCTION LOSS
                                  </p>
                                  <p>
                                      we propose a panoramic image inpainting framework that consists of a Face Generator, a Cube Generator, a side branch, and two discriminators. We use the Cubemap Projection (CMP) format as network input.
                                      The generator employs gated convolutions to distinguish valid pixels from invalid ones, while a side branch is designed utilizing contextual reconstruction (CR) loss to guide the generators to find the most suitable reference patch for inpainting the missing region.

                                  </p>
                                  <p>
                                      <a href="https://vencoders.github.io/PII.html">[Project]</a>
                                      <a href="">[PDF]</a>
                                      <a href="https://github.com/Vencoders/Panoramic-image-inpainting">[Code]</a>
                                  </p>
                              </div>
                          </td>
                      </tr>
		      <tr>
                          <td style="width: 30%">
                              <div class="project-logo">
                                  <a href="https://vencoders.github.io/PU-AT.html"><img src="images/PU-AT1.jpg" style="width: 100%"></a>
                              </div>
                          </td>
                          <td style="width: 70%;">
                              <div class="" >
                                  <p style="font-size: 18pt; text-align: left;">
                                      基于Transformer的点云上采样方法
                                  </p>
                                  <p>
                                      we propose a novel 3D point cloud up-sampling method based on Transformers aiming to explore the po-tential of Transformers in this domain. Initially, a two-step network is employed, transitioning from rough dense point cloud generation to point cloud refinement, with each sub-network focusing on specific objec-tives. The coarse generation network aims to increase point cloud resolution, utilizing multiple densely connected convolutional blocks to extract deep high-dimensional features. To efficiently explore the up-sampling space of point clouds, an innovative integration of upsampling module with Transformer is in-troduced. While the rough generation of point clouds increases the number of points, correction of noise points is essential. In the refinement network, an adaptive refinement module is introduced, capable of au-tonomously aggregating structural information based on local point cloud characteristics.

                                  </p>
                                  <p>
                                      <a href="https://vencoders.github.io/PU-AT.html">[Project]</a>
                                      <a href="">[PDF]</a>
                                      <a href="https://github.com/Vencoders/PU-AT">[Code]</a>
                                  </p>
                              </div>
                          </td>
                      </tr>

                  </table>
                </div>

              </div>
            </div>
          </div>


        </div>
      </div>
    </div>
  </div>
</div>


</div>
</body>
</html>
